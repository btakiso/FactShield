# Product Requirements Document (PRD)
# FactShield: AI-Powered Fake News Detection System

**Version:** 1.0  
**Last Updated:** November 5, 2025  
**Project Type:** Academic AI Course Final Project  
**Team Size:** 1-4 members  
**Timeline:** Now â†’ Before Thanksgiving Break  

---

## ðŸ“‹ Executive Summary

FactShield is a machine learning-based fake news detection system that classifies news articles as "real" or "fake" using supervised learning algorithms. The system's **unique differentiator** is the integration of **sentiment analysis** to detect emotional manipulation patterns commonly found in misinformation.

**Core Value Proposition:**  
Unlike traditional fake news detectors that rely solely on content analysis, FactShield analyzes both textual content AND sentiment patterns to identify manipulation techniques, achieving more nuanced detection.

---

## ðŸŽ¯ Project Objectives

### Primary Goal
Build and train custom machine learning models that classify news articles with demonstrable accuracy using the full repertoire of AI/ML techniques.

### Academic Requirements Alignment
âœ… Apply AI to solve a real-world problem (misinformation)  
âœ… Use multiple ML algorithms and techniques from the course  
âœ… Demonstrate comprehensive approach, process, and analysis  
âœ… Provide detailed evaluation metrics and model comparison  
âœ… Deliver code, technical paper, and presentation  

### Success Criteria
- **Model Performance:** Achieve â‰¥90% accuracy on test set
- **Multi-Model Comparison:** Implement and compare 3 different algorithms
- **Sentiment Integration:** Demonstrate sentiment features contribution to classification
- **Reproducibility:** Fully documented, runnable code
- **Academic Rigor:** Comprehensive analysis and evaluation

---

## ðŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FactShield System                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Data       â”‚      â”‚   Feature    â”‚      â”‚  Model   â”‚ â”‚
â”‚  â”‚  Processing  â”‚ â”€â”€â”€â–º â”‚  Engineering â”‚ â”€â”€â”€â–º â”‚ Training â”‚ â”‚
â”‚  â”‚   Pipeline   â”‚      â”‚   Pipeline   â”‚      â”‚ Pipeline â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                      â”‚                    â”‚      â”‚
â”‚         â–¼                      â–¼                    â–¼      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚            Sentiment Analysis Module                 â”‚ â”‚
â”‚  â”‚  (Extracts emotional manipulation patterns)          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                             â”‚                              â”‚
â”‚                             â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         Ensemble Prediction Engine                   â”‚ â”‚
â”‚  â”‚  (Combines multiple models for final decision)       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                             â”‚                              â”‚
â”‚                             â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚      Evaluation & Visualization Dashboard            â”‚ â”‚
â”‚  â”‚  (Metrics, confusion matrices, feature importance)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ› ï¸ Technology Stack

### Core Language
- **Python 3.10+** (Industry standard for ML/AI)

### Machine Learning & Data Science
```python
# Core ML Framework
scikit-learn==1.5.2          # Traditional ML algorithms
numpy==1.26.4                 # Numerical computing
pandas==2.2.3                 # Data manipulation

# Deep Learning (Optional Advanced Phase)
tensorflow==2.17.0            # Neural networks
transformers==4.45.0          # BERT/RoBERTa models

# NLP & Text Processing
nltk==3.9.1                   # Natural Language Toolkit
spacy==3.8.2                  # Advanced NLP
textblob==0.18.0              # Sentiment analysis

# Visualization & Analysis
matplotlib==3.9.2             # Plotting
seaborn==0.13.2               # Statistical visualization
plotly==5.24.1                # Interactive charts
```

### Development Tools
```python
jupyter==1.1.1                # Notebook environment
ipykernel==6.29.5             # Jupyter kernel
pytest==8.3.3                 # Testing framework
black==24.10.0                # Code formatting
```

### Optional (If Building Web Interface)
```python
flask==3.0.3                  # Lightweight web framework
streamlit==1.39.0             # Rapid ML app development
```

**Why This Stack?**
- âœ… Meets academic requirements (demonstrates ML fundamentals)
- âœ… Industry-standard tools
- âœ… Excellent documentation and community support
- âœ… Runs on any platform (Windows/Mac/Linux)
- âœ… Free and open-source

---

## ðŸ“Š Dataset Selection

### Primary Dataset: ISOT Fake News Dataset
**Source:** https://www.uvic.ca/ecs/ece/isot/datasets/fake-news/index.php  
**Alternative:** https://www.kaggle.com/datasets/clementgautier/fake-and-real-news-dataset

#### Dataset Specifications
```
Total Articles: 44,898
â”œâ”€â”€ Real News: 21,417 articles (Reuters.com, 2016-2017)
â””â”€â”€ Fake News: 23,481 articles (Various sources, 2016-2017)

Columns:
- title        : Article headline
- text         : Full article content
- subject      : Article category (politics, news, etc.)
- date         : Publication date
- label        : 0 (Real) or 1 (Fake)
```

#### Why This Dataset?
âœ… **Balanced Classes:** Nearly 50/50 split prevents model bias  
âœ… **Real-World Data:** Actual news articles, not synthetic  
âœ… **Sufficient Size:** 44K+ articles for robust training  
âœ… **Academic Use:** Widely cited in research papers  
âœ… **Clean Format:** Well-structured CSV files  
âœ… **Diverse Content:** Multiple news categories  

#### Dataset Limitations (To Address in Paper)
- âš ï¸ Temporal constraint (2016-2017) - may not capture recent trends
- âš ï¸ English-only content
- âš ï¸ Potential source bias

### Backup Dataset Options
1. **LIAR Dataset** (12,836 statements with 6-way classification)
2. **FakeNewsNet** (Social context + news content)
3. **BuzzFeed Political News Dataset** (Journalist-verified labels)

---

## ðŸ”¬ Machine Learning Approach

### Phase 1: Baseline Models (Traditional ML)

#### 1.1 Logistic Regression
```python
Pros: Interpretable, linear model, feature importance
Use Case: Understand feature impact, fast training
Expected Accuracy: 92-95%
```

#### 1.2 Random Forest
```python
Pros: Handles non-linear patterns, robust to overfitting
Use Case: Capture complex relationships
Expected Accuracy: 94-97%
```

#### 1.3 Support Vector Machine (SVM)
```python
Pros: Effective in high-dimensional space (text)
Use Case: Maximize margin between classes, often best for text
Expected Accuracy: 93-96%
```

### Phase 2: Advanced Models (Optional)

#### 2.1 LSTM Neural Network
```python
Pros: Sequential text understanding, long-range dependencies
Use Case: Capture narrative structure
Expected Accuracy: 90-92%
```

#### 2.2 BERT Fine-Tuning
```python
Pros: State-of-the-art language understanding
Use Case: Maximum accuracy (if time permits)
Expected Accuracy: 93-95%
```

### Phase 3: Sentiment Analysis Integration (OUR UNIQUE CONTRIBUTION)

#### Sentiment Features to Extract
```python
1. Polarity Score (-1 to 1)
   - Fake news tends toward extreme sentiment

2. Subjectivity Score (0 to 1)
   - Fake news often more subjective/opinionated

3. Emotion Distribution
   - Fear, anger, joy, surprise ratios
   - Fake news exploits specific emotions

4. Sentiment Volatility
   - How much sentiment changes within article
   - Fake news may have inconsistent tone

5. Sensationalism Indicators
   - Excessive punctuation (!!!)
   - ALL CAPS words frequency
   - Clickbait patterns
```

#### Hypothesis to Test
**H1:** Fake news articles exhibit more extreme sentiment polarity than real news  
**H2:** Fake news has higher subjectivity scores  
**H3:** Adding sentiment features improves model accuracy by â‰¥3%  

---

## ðŸ“ Feature Engineering Pipeline

### Text Preprocessing Steps
```python
1. Lowercasing
   "BREAKING NEWS!" â†’ "breaking news!"

2. Remove URLs
   "Visit https://example.com" â†’ "Visit"

3. Remove Special Characters
   "Hello!!!" â†’ "Hello"

4. Tokenization
   "Fake news spreads" â†’ ["Fake", "news", "spreads"]

5. Stop Word Removal
   ["Fake", "news", "spreads"] â†’ ["Fake", "news", "spreads"]
   # Keep meaningful words for context

6. Lemmatization
   ["running", "ran", "runs"] â†’ ["run", "run", "run"]
```

### Feature Extraction Methods

#### Method 1: TF-IDF (Primary)
```python
Term Frequency-Inverse Document Frequency
- Captures word importance across corpus
- Creates sparse matrix representation
- Parameters: max_features=5000, ngram_range=(1,2)
```

#### Method 2: Word Embeddings (Advanced)
```python
Word2Vec / GloVe
- Dense vector representations
- Captures semantic relationships
- Dimension: 100-300
```

### Engineered Features
```python
# Content-based features
- Article length (word count)
- Average sentence length
- Unique word ratio (vocabulary richness)
- Readability scores (Flesch-Kincaid)
- Named entity count (people, places, organizations)

# Sentiment features (YOUR CONTRIBUTION)
- Polarity score
- Subjectivity score
- Emotion distribution
- Sentiment volatility
- Sensationalism score

# Metadata features
- Publication day/time patterns
- Subject category encoding
```

---

## ðŸ“Š Evaluation Methodology

### Train/Validation/Test Split
```
Total Dataset: 44,898 articles
â”œâ”€â”€ Training Set (70%): 31,429 articles
â”œâ”€â”€ Validation Set (15%): 6,735 articles
â””â”€â”€ Test Set (15%): 6,734 articles
```

### Evaluation Metrics

#### Primary Metrics
```python
1. Accuracy = (TP + TN) / Total
   Target: â‰¥85%

2. Precision = TP / (TP + FP)
   How many predicted fakes are actually fake?
   Target: â‰¥85%

3. Recall = TP / (TP + FN)
   How many actual fakes did we catch?
   Target: â‰¥85%

4. F1-Score = 2 * (Precision * Recall) / (Precision + Recall)
   Balanced measure
   Target: â‰¥85%
```

#### Secondary Metrics
```python
5. ROC-AUC Score
   Measures model's ranking ability
   Target: â‰¥0.90

6. Confusion Matrix
   Visualize true/false positives/negatives

7. Classification Report
   Per-class precision, recall, F1
```

### Model Comparison Framework
```python
Create comparison table with TF-IDF + Sentiment Features:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model        â”‚ Accuracy â”‚ Precision â”‚ Recall â”‚ F1      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Log Reg      â”‚  92.7%   â”‚   91.9%   â”‚ 93.2%  â”‚  92.5%  â”‚
â”‚ Random Forestâ”‚  94.3%   â”‚   93.7%   â”‚ 94.8%  â”‚  94.2%  â”‚
â”‚ SVM          â”‚  93.1%   â”‚   92.4%   â”‚ 93.9%  â”‚  93.1%  â”‚
â”‚ LSTM (opt)   â”‚  95.5%   â”‚   94.8%   â”‚ 96.1%  â”‚  95.4%  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

All models utilize:
- 5,000 TF-IDF features (word/phrase importance)
- 3 Sentiment features (polarity, subjectivity, sensationalism)
- Total: 5,003 features per article
```

### Cross-Validation
- **Method:** 5-fold cross-validation
- **Purpose:** Ensure model generalizes well
- **Report:** Mean accuracy Â± standard deviation

---

## ðŸ—‚ï¸ Project Structure

```
factshield/
â”‚
â”œâ”€â”€ README.md                          # Project overview and setup
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ .gitignore                         # Ignore data files, models, etc.
â”‚
â”œâ”€â”€ data/                              # Dataset directory
â”‚   â”œâ”€â”€ raw/                           # Original downloaded data
â”‚   â”‚   â”œâ”€â”€ Fake.csv
â”‚   â”‚   â””â”€â”€ True.csv
â”‚   â”œâ”€â”€ processed/                     # Cleaned and preprocessed
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â”œâ”€â”€ val.csv
â”‚   â”‚   â””â”€â”€ test.csv
â”‚   â””â”€â”€ README.md                      # Dataset documentation
â”‚
â”œâ”€â”€ notebooks/                         # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb     # EDA and visualization
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb        # Data cleaning pipeline
â”‚   â”œâ”€â”€ 03_feature_engineering.ipynb  # Feature extraction
â”‚   â”œâ”€â”€ 04_sentiment_analysis.ipynb   # Sentiment feature creation
â”‚   â”œâ”€â”€ 05_baseline_models.ipynb      # Traditional ML models
â”‚   â”œâ”€â”€ 06_advanced_models.ipynb      # Deep learning (optional)
â”‚   â”œâ”€â”€ 07_model_evaluation.ipynb     # Comprehensive evaluation
â”‚   â””â”€â”€ 08_final_results.ipynb        # Final analysis & visualizations
â”‚
â”œâ”€â”€ src/                               # Source code modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py                 # Dataset loading functions
â”‚   â”‚   â””â”€â”€ preprocessor.py           # Text preprocessing
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_features.py          # TF-IDF, embeddings
â”‚   â”‚   â”œâ”€â”€ sentiment_features.py     # Sentiment extraction
â”‚   â”‚   â””â”€â”€ feature_engineering.py    # Custom features
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ baseline_models.py        # NB, LR, RF, SVM
â”‚   â”‚   â”œâ”€â”€ deep_models.py            # LSTM, BERT
â”‚   â”‚   â””â”€â”€ ensemble.py               # Ensemble methods
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py                # Evaluation functions
â”‚   â”‚   â””â”€â”€ visualizations.py         # Plotting functions
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py                # Utility functions
â”‚
â”œâ”€â”€ models/                            # Saved trained models
â”‚   â”œâ”€â”€ logistic_regression_model.pkl
â”‚   â”œâ”€â”€ random_forest_model.pkl
â”‚   â”œâ”€â”€ svm_model.pkl
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl          # Saved TF-IDF vectorizer
â”‚   â””â”€â”€ X_*_features.npz              # Feature matrices
â”‚
â”œâ”€â”€ results/                           # Evaluation results
â”‚   â”œâ”€â”€ metrics/                      # CSV files with metrics
â”‚   â”œâ”€â”€ visualizations/               # Plots and charts
â”‚   â””â”€â”€ comparison_report.md          # Model comparison
â”‚
â”œâ”€â”€ reports/                           # Academic deliverables
â”‚   â”œâ”€â”€ final_paper.md                # Technical paper
â”‚   â”œâ”€â”€ presentation.pptx             # Final presentation
â”‚   â””â”€â”€ figures/                      # Paper figures
â”‚
â”œâ”€â”€ app/                               # Optional web interface
â”‚   â”œâ”€â”€ app.py                        # Flask/Streamlit app
â”‚   â”œâ”€â”€ templates/                    # HTML templates
â”‚   â””â”€â”€ static/                       # CSS, JS, images
â”‚
â””â”€â”€ tests/                             # Unit tests
    â”œâ”€â”€ test_preprocessing.py
    â”œâ”€â”€ test_features.py
    â””â”€â”€ test_models.py
```

---

## ðŸ“… Development Timeline

### Week 1: Foundation (Nov 5-8)
**Days 1-2: Setup & Data Exploration**
- [ ] Set up Python environment and install dependencies
- [ ] Download and explore ISOT dataset
- [ ] Create comprehensive EDA notebook
- [ ] Visualize class distribution, text length, word frequencies
- [ ] Document initial insights

**Days 3-4: Data Preprocessing**
- [ ] Build text cleaning pipeline
- [ ] Implement preprocessing functions
- [ ] Create train/val/test splits
- [ ] Validate data quality
- [ ] Save processed datasets

### Week 2: Model Development (Nov 9-15)
**Days 5-7: Feature Engineering**
- [ ] Implement TF-IDF vectorization
- [ ] Extract basic content features
- [ ] Build sentiment analysis module
- [ ] Create feature combination pipeline
- [ ] Validate feature quality

**Days 8-10: Model Training**
- [ ] Train Logistic Regression
- [ ] Train Random Forest
- [ ] Train SVM
- [ ] Evaluate and compare all models

**Days 11-12: Sentiment Integration**
- [ ] Add sentiment features to pipeline
- [ ] Retrain all models with sentiment
- [ ] Measure performance improvement
- [ ] Statistical significance testing
- [ ] Document findings

### Week 3: Finalization (Nov 16-22)
**Days 13-15: Advanced Models (Optional)**
- [ ] Implement LSTM network
- [ ] Fine-tune BERT (if time permits)
- [ ] Compare with baseline models

**Days 16-18: Evaluation & Analysis**
- [ ] Comprehensive model evaluation
- [ ] Create visualizations (confusion matrices, ROC curves)
- [ ] Feature importance analysis
- [ ] Error analysis
- [ ] Generate comparison tables

**Days 19-21: Deliverables**
- [ ] Write technical paper
- [ ] Create presentation slides
- [ ] Code cleanup and documentation
- [ ] Record demo video (optional)
- [ ] Final testing

**Day 22: Buffer & Submission**
- [ ] Final review
- [ ] Submit before Thanksgiving

---

## ðŸ“ Deliverables

### 1. Code Repository
```
GitHub Repository Contents:
âœ… All source code (clean, commented)
âœ… Jupyter notebooks (executed with outputs)
âœ… README with setup instructions
âœ… requirements.txt
âœ… Saved models (or instructions to train)
âœ… Sample predictions
```

### 2. Technical Paper (8-12 pages)
```markdown
Recommended Structure:

1. Abstract (250 words)
   - Problem, approach, key findings

2. Introduction (1-2 pages)
   - Problem statement
   - Motivation
   - Research questions
   - Contributions

3. Related Work (1 page)
   - Brief literature review
   - Existing approaches
   - Your innovation

4. Methodology (3-4 pages)
   - Dataset description
   - Preprocessing pipeline
   - Feature engineering
   - Sentiment analysis approach
   - Models implemented
   - Evaluation metrics

5. Results (2-3 pages)
   - Model performance comparison
   - Sentiment feature impact
   - Visualizations (confusion matrices, ROC curves)
   - Statistical analysis
   - Error analysis

6. Discussion (1-2 pages)
   - Findings interpretation
   - Sentiment analysis effectiveness
   - Limitations
   - Real-world applications

7. Conclusion & Future Work (1 page)
   - Summary
   - Key takeaways
   - Future improvements

8. References
   - Dataset citations
   - Papers referenced
   - Libraries used
```

### 3. Presentation (10-15 minutes)
```
Slide Structure:

1. Title Slide
   - Project name, team members

2. Problem Statement (1 slide)
   - Why fake news detection matters

3. Approach Overview (1 slide)
   - System architecture diagram

4. Dataset & Preprocessing (1-2 slides)
   - Dataset statistics
   - Preprocessing pipeline

5. Feature Engineering (2 slides)
   - Text features
   - Sentiment features (your contribution)

6. Models Implemented (2 slides)
   - Algorithms used
   - Training approach

7. Results (3-4 slides)
   - Model comparison table
   - Accuracy charts
   - Confusion matrices
   - Sentiment impact visualization

8. Demo (1-2 slides or live demo)
   - Show prediction on sample article

9. Key Findings (1 slide)
   - Main insights

10. Conclusion (1 slide)
    - Summary and future work

Total: 12-15 slides
```

---

## ðŸŽ¯ Success Criteria

### Minimum Viable Project (MVP)
âœ… Train 3 different ML models (Log Reg, Random Forest, SVM)
âœ… Achieve â‰¥90% accuracy on test set  
âœ… Implement sentiment analysis features  
âœ… Show model comparison with metrics  
âœ… Complete code documentation  
âœ… Submit technical paper  
âœ… Deliver presentation  

### Excellent Project (Target)
âœ… All MVP criteria  
âœ… Achieve â‰¥93% accuracy  
âœ… Demonstrate sentiment features contribution to classification
âœ… Comprehensive error analysis  
âœ… Statistical significance testing  
âœ… Feature importance analysis  
âœ… Optional: Deep learning models (LSTM/BERT)  

### Outstanding Project (Stretch Goals)
âœ… All excellent project criteria  
âœ… Novel sentiment features or techniques  
âœ… Ensemble methods  
âœ… Real-time news URL analysis  
âœ… Publication-ready visualizations  
âœ… Reproducible research (containerized)  

---

## ðŸš§ Technical Challenges & Mitigation

### Challenge 1: Dataset Size & Memory
**Problem:** 44K articles may be memory-intensive  
**Solution:**
- Use batch processing
- Implement data generators
- Limit TF-IDF features to top 5000

### Challenge 2: Training Time
**Problem:** Models may take hours to train  
**Solution:**
- Start with smaller sample for testing
- Use stratified sampling
- Implement checkpointing
- Train overnight if needed

### Challenge 3: Sentiment Analysis Accuracy
**Problem:** Off-the-shelf sentiment tools may be inaccurate  
**Solution:**
- Use multiple sentiment libraries
- Ensemble sentiment scores
- Manual validation on sample

### Challenge 4: Model Overfitting
**Problem:** Models may memorize training data  
**Solution:**
- Use cross-validation
- Implement regularization
- Monitor train vs. validation accuracy
- Use dropout in neural networks

### Challenge 5: Time Constraint
**Problem:** 3 weeks is tight for full implementation  
**Solution:**
- Focus on MVP first
- Prioritize baseline models
- Deep learning is optional
- Use provided timeline as guide

---

## ðŸ“– Learning Resources

### Required Reading
1. **Scikit-learn Documentation**
   - https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html

2. **NLTK Book**
   - https://www.nltk.org/book/

3. **TextBlob Sentiment Analysis**
   - https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis

### Recommended Tutorials
1. **Fake News Detection with Python** (DataCamp)
   - https://www.datacamp.com/tutorial/scikit-learn-fake-news

2. **Text Classification with Scikit-learn**
   - https://realpython.com/python-keras-text-classification/

### Research Papers (For Paper's Related Work Section)
1. Ahmed et al. (2017) - "Detection of Online Fake News Using N-Gram Analysis"
2. PÃ©rez-Rosas et al. (2018) - "Automatic Detection of Fake News"
3. Shu et al. (2020) - "Combating Disinformation in a Social Media Age"

---

## ðŸ”’ Academic Integrity Guidelines

### Allowed
âœ… Using publicly available datasets  
âœ… Using standard ML libraries (scikit-learn, etc.)  
âœ… Referencing tutorials and documentation  
âœ… Discussing general approaches with classmates  
âœ… Using Stack Overflow for debugging  

### Not Allowed
âŒ Copying code from GitHub projects without attribution  
âŒ Using pre-trained fake news models without training your own  
âŒ Having someone else write your code  
âŒ Submitting work from previous semesters  

### Proper Attribution
- Cite dataset source in paper
- Reference papers you read
- Acknowledge libraries used
- Comment borrowed code snippets with source

---

## ðŸŽ“ Questions for our Professor Dr. Lingjie Liu

Before starting implementation, confirm:

1. **Scope Clarification**
   - "Should we build custom ML models or is API usage acceptable?"
   - "Are 3-4 different algorithms sufficient for comparison?"

2. **Deliverable Format**
   - "What format do you prefer for the final paper (PDF, Word, LaTeX)?"
   - "Should code be submitted as Jupyter notebooks or structured package?"

3. **Evaluation Criteria**
   - "How much weight is given to model accuracy vs. methodology documentation?"
   - "Is sentiment analysis integration sufficient for unique contribution?"

4. **Technical Details**
   - "Are there specific ML techniques you expect to see?"
   - "Should we implement deep learning or focus on traditional ML?"

5. **Timeline**
   - "What is the exact submission deadline before Thanksgiving?"
   - "Will there be presentations in class or pre-recorded?"

---

## ðŸš€ Getting Started (Next Steps)

1. **Read this entire PRD** (30 minutes)
2. **Ask professor the questions listed above** (before next class)
3. **Set up development environment** (1 hour)
4. **Download and explore dataset** (2 hours)
5. **Start Week 1 tasks** (follow timeline)

---

## ðŸ“ž Project Support

### When You Get Stuck
1. Review this PRD for guidance
2. Check documentation for libraries
3. Search Stack Overflow
4. Ask professor during office hours
5. Discuss with teammates (if team project)

### Regular Check-ins
- **Daily:** Review progress against timeline
- **Weekly:** Complete milestone deliverables
- **Continuous:** Document decisions and findings

---

## âœ… Definition of Done

This project is complete when:

- [ ] All code is written, tested, and documented
- [ ] All models trained and evaluated
- [ ] Technical paper written and proofread
- [ ] Presentation slides created
- [ ] Repository is clean and organized
- [ ] README has clear setup instructions
- [ ] All deliverables submitted before deadline
- [ ] You can confidently explain every aspect of your work

---

## ðŸ“Œ Key Principles

Throughout this project, remember:

1. **Process > Results:** Document your thinking, not just outcomes
2. **Iterate Fast:** Build MVP first, enhance later
3. **Stay Focused:** Stick to this PRD, avoid feature creep
4. **Be Honest:** Acknowledge limitations in your paper
5. **Learn Deeply:** Understand why methods work, not just how

---

**This PRD is your roadmap. Refer to it daily. Update it if requirements change. Good luck! ðŸš€**

